# OpenAugI (Open Augmented Intelligence)

(pronounced "Open Augie")

Augmented Intelligence for your second brain.

Sign up for more at [OpenAugi](https://openaugi.com)

Check out [OpenAugI Voice on Obsidian](https://github.com/bitsofchris/openaugi-obsidian-voice-plugin)


## Mission

Building an open, privacy-first ecosystem of knowledge tools that augment human intelligence while keeping users in control of their data and ideas.


## Core Values
- **Privacy as Foundation**: Take back your data. Write your own story.
- **Composable Ecosystem**: Building blocks that work together. Innovation happens quick, swap pieces when needed.
- **Augment, Stay Human**: Amplify your unique self, use LLMs as a tool to go faster.

## Current Projects

We're in the early stages of development, working on:

- Taking a voice note and hierarchically distilling it to atomic notes
- De-duplicating by embedding clustering
- Merging it to an existing knowledge graph (explict links and semantically)

Check the [`experiments/`](./experiments/) folder for our latest prototypes.

## Getting Involved

We're building in public and welcome contributors who share our values.

- üåü **Star and watch** this repository to stay up to date
- üó£Ô∏è **Join the discussion** on [Discord](https://discord.gg/d26BVBrnRP) or on [Github](https://github.com/openaugi/openaugi/issues)
- üì£ **Spread the word** about our mission

Follow our progress on @bitsofchris [Youtube](https://www.youtube.com/@bitsofchris).


# Knowledge Distillation Engine - Research Preview Repo

This repository showcases my research into knowledge distillation and automated learning systems. 

UPDATE 2025-07-07: After noticing my work being cited by GPT Deep Research, I've decided to transition this project to a research preview model while I explore the commercial potential of my approaches. 

## What's Here
- **Proof of concept implementations** - Working examples of core techniques
- **Educational utilities** - Tools and helpers for knowledge distillation workflows
- **Research insights** - Findings and learnings from my experiments
- **Documentation** - Guides on general knowledge distillation principles

## What's Not Here
- **Production implementations** - My latest algorithmic improvements and optimizations
- **Proprietary techniques** - Novel approaches I'm developing privately
- **Active development** - Current experimental work happens in private repositories

## Why This Approach?
I believe in building in public and sharing knowledge, but I also recognize when research has commercial value. This preview model lets me share enough to work with other like minded folks but also protect what is being created from being pillaged by big companies.

## License Update
**Note**: This project has transitioned from MIT to GPL v3 license to better align with its research preview status. This ensures that any derivatives remain open source while I explore commercial applications of the underlying techniques. For commercial licensing inquiries, please reach out directly.

## For Researchers & Developers
If you're working in knowledge distillation or related fields, I'm open to collaboration and always interested in discussing novel approaches. Feel free to reach out through issues or my other channels.

---

*This project represents ongoing research into knowledge distillation systems. While core techniques remain private, I'm committed to advancing the field through selective sharing and collaboration.*

