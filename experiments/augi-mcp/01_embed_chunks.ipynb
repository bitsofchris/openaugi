{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/conversations.json\n",
      "Type: <class 'list'>\n",
      "Size: 690\n",
      "\n",
      "=== JSON Structure ===\n",
      "[{'account': 'dict(1 items)',\n",
      "  'chat_messages': 'list(8 items)',\n",
      "  'created_at': 'str: 2024-05-28T13:05:48.783430Z...',\n",
      "  'name': 'str: Monetizing Creativity with Blockchain AGI...',\n",
      "  'updated_at': 'str: 2024-05-28T13:12:45.115241Z...',\n",
      "  'uuid': 'str: b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312...'},\n",
      " '... and 689 more items']\n",
      "\n",
      "=== First item details ===\n",
      "{'account': {'uuid': '2fc29045-9a0b-488f-af46-48e235f655ea'},\n",
      " 'chat_messages': [{...}, {...}, {...}, {...}, {...}, {...}, {...}, {...}],\n",
      " 'created_at': '2024-05-28T13:05:48.783430Z',\n",
      " 'name': 'Monetizing Creativity with Blockchain AGI',\n",
      " 'updated_at': '2024-05-28T13:12:45.115241Z',\n",
      " 'uuid': 'b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "# Method 1: Basic JSON loading\n",
    "def load_and_explore_json(file_path):\n",
    "    \"\"\"Load JSON file and explore its structure\"\"\"\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Basic exploration\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Type: {type(data)}\")\n",
    "    print(f\"Size: {len(data) if hasattr(data, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Method 2: Pretty print structure\n",
    "def explore_json_structure(data, max_depth=2, current_depth=0):\n",
    "    \"\"\"Recursively explore JSON structure\"\"\"\n",
    "    \n",
    "    if current_depth > max_depth:\n",
    "        return \"...\"\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        result = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                result[key] = f\"{type(value).__name__}({len(value)} items)\"\n",
    "            else:\n",
    "                result[key] = f\"{type(value).__name__}: {str(value)[:50]}...\"\n",
    "        return result\n",
    "    \n",
    "    elif isinstance(data, list):\n",
    "        if len(data) == 0:\n",
    "            return \"Empty list\"\n",
    "        elif len(data) == 1:\n",
    "            return [explore_json_structure(data[0], max_depth, current_depth + 1)]\n",
    "        else:\n",
    "            return [\n",
    "                explore_json_structure(data[0], max_depth, current_depth + 1),\n",
    "                f\"... and {len(data) - 1} more items\"\n",
    "            ]\n",
    "    \n",
    "    else:\n",
    "        return f\"{type(data).__name__}: {str(data)[:50]}...\"\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"data/conversations.json\"\n",
    "\n",
    "# Load the data\n",
    "data = load_and_explore_json(file_path)\n",
    "\n",
    "# Explore structure\n",
    "print(\"\\n=== JSON Structure ===\")\n",
    "structure = explore_json_structure(data)\n",
    "pprint.pprint(structure, width=80, depth=3)\n",
    "\n",
    "# If it's a list, show first few items\n",
    "if isinstance(data, list):\n",
    "    print(f\"\\n=== First item details ===\")\n",
    "    if len(data) > 0:\n",
    "        pprint.pprint(data[0], width=80, depth=2)\n",
    "\n",
    "# If it's a dict, show keys and sample values\n",
    "elif isinstance(data, dict):\n",
    "    print(f\"\\n=== Dictionary keys ===\")\n",
    "    for key, value in data.items():\n",
    "        print(f\"{key}: {type(value).__name__}\")\n",
    "        if isinstance(value, (list, dict)) and len(value) > 0:\n",
    "            print(f\"  Sample: {str(value)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse into Llamaindex document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "from llama_index.core import Document\n",
    "\n",
    "def parse_timestamp_to_int(timestamp_str):\n",
    "    \"\"\"Convert ISO timestamp to integer\"\"\"\n",
    "    if not timestamp_str:\n",
    "        return 0\n",
    "    try:\n",
    "        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
    "        return int(dt.timestamp() * 1000000)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def create_documents_per_turn(conversations_data: List[Dict]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create one document for each turn/message in conversations.\n",
    "    \n",
    "    Args:\n",
    "        conversations_data: List of conversation dictionaries from JSON\n",
    "        \n",
    "    Returns:\n",
    "        List of LlamaIndex Document objects (one per message)\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for conversation in conversations_data:\n",
    "        thread_id = conversation.get('uuid', 'unknown')\n",
    "        conversation_name = conversation.get('name', 'Untitled Conversation')\n",
    "        conversation_created = conversation.get('created_at')\n",
    "        \n",
    "        # Process each message as a separate document\n",
    "        for turn_idx, message in enumerate(conversation.get('chat_messages', [])):\n",
    "            # Extract text content from message\n",
    "            text_content = \"\"\n",
    "            content = message.get('content', [])\n",
    "            \n",
    "            if isinstance(content, list):\n",
    "                text_parts = []\n",
    "                for content_item in content:\n",
    "                    if isinstance(content_item, dict) and 'text' in content_item:\n",
    "                        text_parts.append(content_item['text'])\n",
    "                text_content = \"\\n\".join(text_parts)\n",
    "            elif isinstance(content, str):\n",
    "                text_content = content\n",
    "            elif isinstance(content, dict) and 'text' in content:\n",
    "                text_content = content['text']\n",
    "            \n",
    "            # Only create document if there's actual text content\n",
    "            if text_content.strip():\n",
    "                 # Create deterministic ID\n",
    "                created_at = message.get('created_at', '')\n",
    "                timestamp_int = parse_timestamp_to_int(created_at)\n",
    "                doc_id = f\"{thread_id}-{turn_idx:03d}-{timestamp_int}\"\n",
    "                \n",
    "                docs.append(\n",
    "                    Document(\n",
    "                        text=text_content,\n",
    "                        metadata={\n",
    "                            \"thread\": thread_id,\n",
    "                            \"thread_name\": conversation_name,\n",
    "                            \"role\": message.get('sender', 'unknown'),  # \"human\" / \"assistant\"\n",
    "                            \"turn_index\": turn_idx,\n",
    "                            \"conversation_created\": conversation_created,\n",
    "                            \"message_created\": message.get('created_at'),\n",
    "                            \"source\": \"conversations.json\",\n",
    "                            \"doc_id\": doc_id\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    print(f\"Created {len(docs)} documents from individual turns\")\n",
    "    return docs\n",
    "\n",
    "# Create documents - one per turn/message\n",
    "docs = create_documents_per_turn(data)\n",
    "\n",
    "# Show statistics\n",
    "if docs:\n",
    "    print(f\"\\nDocument Statistics:\")\n",
    "    print(f\"Total documents: {len(docs)}\")\n",
    "    \n",
    "    # Count by role\n",
    "    role_counts = {}\n",
    "    for doc in docs:\n",
    "        role = doc.metadata.get('role', 'unknown')\n",
    "        role_counts[role] = role_counts.get(role, 0) + 1\n",
    "    \n",
    "    print(f\"Messages by role: {role_counts}\")\n",
    "    \n",
    "    # Show sample document\n",
    "    print(f\"\\nSample document:\")\n",
    "    sample_doc = docs[0]\n",
    "    print(f\"Role: {sample_doc.metadata['role']}\")\n",
    "    print(f\"Thread: {sample_doc.metadata['thread_name']}\")\n",
    "    print(f\"Text length: {len(sample_doc.text)}\")\n",
    "    print(f\"Text preview: {sample_doc.text[:200]}...\")\n",
    "    print(f\"Full metadata: {sample_doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uuid', 'name', 'created_at', 'updated_at', 'account', 'chat_messages'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='200033be-6255-4d84-ba83-25fb9634fd46', embedding=None, metadata={'thread': 'b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312', 'thread_name': 'Monetizing Creativity with Blockchain AGI', 'role': 'human', 'turn_index': 0, 'conversation_created': '2024-05-28T13:05:48.783430Z', 'message_created': '2024-05-28T13:07:32.757438Z', 'source': 'conversations.json'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Right now can I scope this down or pick a single focused that’s in this area I can turn into a Soloprenuer or small startup.\\n\\nI was thinking exploring LLms for writers and using ai to define and own a writing niche.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and store in DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x11b9f42f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, datetime, uuid\n",
    "\n",
    "\n",
    "db = duckdb.connect(\"augmcp_v0.duckdb\")\n",
    "db.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS raw_chunks(\n",
    "  doc_id  TEXT PRIMARY KEY,\n",
    "  thread_id    TEXT,\n",
    "  role      TEXT,\n",
    "  ts_ingest TIMESTAMP,\n",
    "  content   TEXT,\n",
    "  embedding DOUBLE[]\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Robust\n",
    "- resumes\n",
    "- handles errors\n",
    "- parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 already-embedded docs; skipping them.\n",
      "Embedding 11284 new documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 11284/11284 [26:27<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import duckdb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "load_dotenv(\"/Users/chris/repos/openaugi/keys.env\")\n",
    "embedder = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "DB_PATH   = \"augmcp_v0.duckdb\"\n",
    "TABLE_NAME = \"raw_chunks\"\n",
    "MAX_TEXT_LEN = 8192     # truncate to first 8k chars\n",
    "MAX_WORKERS  = 5        # tweak based on your notebook / rate limits\n",
    "\n",
    "def get_existing_doc_ids(conn):\n",
    "    \"\"\"Fetch all doc_ids already in the DB so we can skip them.\"\"\"\n",
    "    try:\n",
    "        rows = conn.execute(f\"SELECT DISTINCT doc_id FROM {TABLE_NAME}\").fetchall()\n",
    "        return {row[0] for row in rows}\n",
    "    except duckdb.CatalogException:\n",
    "        # If table doesn’t exist yet\n",
    "        return set()\n",
    "\n",
    "def _embed_chunk(doc):\n",
    "    \"\"\"\n",
    "    Truncate text, call the embedding endpoint, and return\n",
    "    the full record ready for insertion.\n",
    "    \"\"\"\n",
    "    text = doc.text[:MAX_TEXT_LEN]\n",
    "    emb  = embedder.get_text_embedding(text)\n",
    "    return (\n",
    "        doc.metadata[\"doc_id\"],\n",
    "        doc.metadata[\"thread\"],\n",
    "        doc.metadata[\"role\"],\n",
    "        datetime.datetime.fromisoformat(doc.metadata[\"message_created\"]),\n",
    "        text,\n",
    "        emb\n",
    "    )\n",
    "\n",
    "def process_documents_resumable(docs):\n",
    "    # ——— 1. Open a single DuckDB connection on the main thread ———\n",
    "    conn = duckdb.connect(DB_PATH)\n",
    "    existing = get_existing_doc_ids(conn)\n",
    "    print(f\"Found {len(existing)} already-embedded docs; skipping them.\")\n",
    "\n",
    "    # ——— 2. Filter out docs that are already in the DB ———\n",
    "    to_process = [d for d in docs if d.metadata[\"thread\"] not in existing]\n",
    "    print(f\"Embedding {len(to_process)} new documents...\")\n",
    "\n",
    "    # ——— 3. Spin up threads to do only the embedding calls ———\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(_embed_chunk, d): d for d in to_process}\n",
    "\n",
    "        # ——— 4. As each future completes, insert its result serially ———\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding\"):\n",
    "            doc = futures[future]\n",
    "            try:\n",
    "                record = future.result()\n",
    "                conn.execute(\n",
    "                    f\"INSERT INTO {TABLE_NAME} VALUES (?,?,?,?,?,?)\",\n",
    "                    record\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed on doc {doc.metadata['thread']}: {e}\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "# ——— Usage ———\n",
    "process_documents_resumable(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is embedded into DuckDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              doc_id  \\\n",
      "0  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-003-17169...   \n",
      "1  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-000-17169...   \n",
      "2  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-004-17169...   \n",
      "3  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-002-17169...   \n",
      "4  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-001-17169...   \n",
      "5  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-006-17169...   \n",
      "6  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-007-17169...   \n",
      "7  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312-005-17169...   \n",
      "8  94040b8e-44fd-4854-9aa3-adc2b65baa5c-000-17168...   \n",
      "9  94040b8e-44fd-4854-9aa3-adc2b65baa5c-005-17168...   \n",
      "\n",
      "                              thread_id       role                  ts_ingest  \\\n",
      "0  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312  assistant 2024-05-28 09:07:32.757438   \n",
      "1  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312      human 2024-05-28 09:07:32.757438   \n",
      "2  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312      human 2024-05-28 09:11:21.515193   \n",
      "3  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312  assistant 2024-05-28 09:06:00.009748   \n",
      "4  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312      human 2024-05-28 09:06:00.009748   \n",
      "5  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312      human 2024-05-28 09:12:45.115241   \n",
      "6  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312  assistant 2024-05-28 09:12:45.115241   \n",
      "7  b4fa5efd-9e1c-4e4b-a8fe-ae1dba7ef312  assistant 2024-05-28 09:11:21.515193   \n",
      "8  94040b8e-44fd-4854-9aa3-adc2b65baa5c      human 2024-05-28 06:25:14.193845   \n",
      "9  94040b8e-44fd-4854-9aa3-adc2b65baa5c  assistant 2024-05-28 06:27:53.702881   \n",
      "\n",
      "                                             content  \\\n",
      "0  Focusing on using large language models (LLMs)...   \n",
      "1  Right now can I scope this down or pick a sing...   \n",
      "2  I have a lot of ideas I want to work on. But I...   \n",
      "3  Building an AGI (Artificial General Intelligen...   \n",
      "4  I had a big idea to build an AGI on the Blockc...   \n",
      "5  How can I find a problem or niche in markets? ...   \n",
      "6  Finding a problem or niche in the market for a...   \n",
      "7  If your primary goal is to start generating in...   \n",
      "8  I'm learning how to train LLMs.\\n\\ntoday it se...   \n",
      "9  Curriculum learning is an active area of resea...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.0023773855064064264, 7.478090265067294e-05,...  \n",
      "1  [0.009010972455143929, 0.0027976101264357567, ...  \n",
      "2  [-0.006414270494133234, 0.018262464553117752, ...  \n",
      "3  [0.03863655775785446, -0.009854933246970177, 0...  \n",
      "4  [0.008724294602870941, -0.02699824795126915, -...  \n",
      "5  [-0.010303616523742676, -0.002924482338130474,...  \n",
      "6  [0.020741501823067665, 0.028282858431339264, 0...  \n",
      "7  [-0.006392394192516804, 0.014629892073571682, ...  \n",
      "8  [-0.041347820311784744, 0.03836805373430252, 0...  \n",
      "9  [-0.010804945603013039, -0.008913209661841393,...  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "conn = duckdb.connect('augmcp_v0.duckdb')\n",
    "\n",
    "# Convert query to pandas DataFrame for nice display\n",
    "df = conn.execute(\"SELECT * FROM raw_chunks LIMIT 10\").df()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dim Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x127451c70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = duckdb.connect(\"augmcp_v0.duckdb\")\n",
    "\n",
    "# Read your conversations JSON into a DataFrame\n",
    "convs = pd.read_json(\"data/conversations.json\")\n",
    "threads = convs[[\"uuid\", \"name\"]].rename(\n",
    "    columns={\"uuid\": \"thread_id\", \"name\": \"thread_name\"}\n",
    ")\n",
    "\n",
    "# Write it into DuckDB\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_thread (\n",
    "    thread_id   TEXT PRIMARY KEY,\n",
    "    thread_name TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "conn.register(\"threads_df\", threads)        # temp table backed by pandas\n",
    "conn.execute(\"INSERT OR REPLACE INTO dim_thread SELECT * FROM threads_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE DATA FROM VIEW ===\n",
      "Total rows in vw_chunks_with_name: 11,284\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# 1. Connect to your database\n",
    "conn = duckdb.connect(\"augmcp_v0.duckdb\")\n",
    "\n",
    "# 2. Create (or replace) the view\n",
    "conn.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_chunks_with_name AS\n",
    "SELECT\n",
    "  rc.doc_id,\n",
    "  rc.thread_id,\n",
    "  dt.thread_name,\n",
    "  rc.role,\n",
    "  rc.ts_ingest,\n",
    "  rc.content,\n",
    "  rc.embedding\n",
    "FROM raw_chunks AS rc\n",
    "JOIN dim_thread AS dt\n",
    "  ON rc.thread_id = dt.thread_id\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 5. Sample some rows to confirm the join\n",
    "print(\"\\n=== SAMPLE DATA FROM VIEW ===\")\n",
    "out = conn.execute(\"\"\"\n",
    "SELECT\n",
    "  doc_id,\n",
    "  thread_id,\n",
    "  thread_name,\n",
    "  role,\n",
    "  LEFT(content, 80) AS content_preview\n",
    "FROM vw_chunks_with_name\n",
    "LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "# print(out)\n",
    "\n",
    "num_rows = conn.execute(\"SELECT COUNT(*) FROM vw_chunks_with_name\").fetchone()[0]\n",
    "print(f\"Total rows in vw_chunks_with_name: {num_rows:,}\")\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
